数据工程三个方面：数据多样性、数据质量、数据配比

高级提示策略，包括上下文学习、思维链提示

- 上下文：给出任务描述和一些示例的输入输出就可对新输入得到正确输出
- 思维链（COT） ： 增加得到结果的推理过程：只添加一句Let's think step by step就可实现zero shot COT

LLM训练过程可以分为大规模预训练和指令微调与人类对齐两个阶段
扩展法则（Scaling Law):

## 预训练

### 预训练预处理流程

![image.png](assets/image.png)

我参加了招商银行的数字金融训练营的AI赛道，
在决赛中，我们要做的提高一个1.3B大小，训练token长度为1K的LLAMA 结构的base模型的长文本能力，想要通过继续预训练有效扩展了大语言模型的上下文窗口，

我们关于长文本能力的指标是模型在不同长度的文本上的平均PPL，我们的方案是修改位置编码方式并使用长文本进行增量预训练
我们将模型的位置编码方式修改为了动态NTK插值（本质上是根据输入文本长度 𝑇 动态地更改旋转角度的底数， 从而缩小旋转的角度），使得模型在不经过训练就具有一定的外推能力，为了真正提供长文本建模能力，我们还利用了longlora方法进行了增量预训练，以扩展上下文长度

我负责的主要是预训练数据处理和部分的模型的训练方案设计，
预训练中我们拥有的是中英文的书籍、百科、新闻、代码等共100G的中英文的原始语料，
在进行训练之前，我们需要先预处理来提高训练数据的质量，构造出高质量的长文本数据，从而提高模型能力，
我们的预处理路线是统计过滤+关键词筛选+数据去重。

- 统计过滤：针对维基百科数据，过滤掉任何拥有少于25个UTF-8单词的页面。
- 关键词筛选：构建金融相关的中英文词典，筛选出金融相关数据
- 数据去重： 防止出现重复的输出。采用多阶段、多粒度的方式来实现高效的去重，首先针对数据集和文档级别进行去重，再对句子进行去重

这上面的步骤其实都是对数据的质量进行过滤，由于这是比赛，时间比较紧，所以我们没有再进行后续可能存在的敏感数据这类有关安全问题的处理，

完成数据预处理之后，还需要注意数据配比，我们的书籍、百科、新闻、代码的比例为：
在训练阶段，我们采用多阶段提升的方案进行增量预训练，先采用大量的4K数据训练，之后用少量的8K,16K,32K
来逐渐提升上下文窗口，

2.5T 词元，4K上下文
窗口→ 20B 词元，16K 上下文窗口。

Longlora:
改进长文本的注意力计算耗时的操作，使用了shift short attention, 计算attention时分为n段，那么就只需T/n的长度来处理上下文窗口，如果仅用short attention,不同段之间缺乏交互，把attention分成两部分，一个是原始的short attention, 另一个是在分段上的偏移，比如原始长度8096。0-2047第一段，把下一段的一半即1024-3069作为shfted，然后两者

同时，通过指令微调提升模型的指令遵循能力，成功解决“大海捞针”任务。

## 高效训练技术，

包括 3D 并行训练、激活重计算和混合精度训练。

![image.png](assets/image.png?t=1730823456282)

### 3D 并行训练:

- 数据并行：将数据放在多个gpu上，复制模型参数和优化器状态到每个gpu，平均每个gpu的梯度进行总的参数更新
- 流水线并行：将模型的不同层放在不同GPU上，结合梯度累积进行反向传播和参数更新
- 张量并行：把参数矩阵按行列分解，在不同GPU上进行计算后再合并
- ZeRO: 解决数据并行中的模型冗余问题，将模型的每层参数分配在不同GPU上，每个GPU只保留部分参数，需要时再与其他GPU通信来获得

### 混合精度训练

先保存32位参数FP32原始副本，然后在前向传播和反向传播时用16位FP16进行运算，得到梯度后更新32位参数，实现显存需求减半，大模型中采用BF16（增大了指数位的数量提高了表示范围）

### 层归一化

- LayerNorm
  ![image.png](assets/image.png?t=1730826169559)
- RMSNorm
  为了提高层归一化的训练速度，RMSNorm仅利用激活值总和的均方根RMS(𝒙) 对激活值进行重新缩放
  ![image.png](assets/image.png?t=1730826352390)

### 注意力机制优化

* ​**稀疏注意力**​：通过滑动窗口机制优化，将注意力计算限制在某个token的前`w`个位置，忽略远处不相关的token，使复杂度从`O(T^2)`降低为`O(wT)`，适合长序列任务。
* ​**分组查询与多查询注意力**​：在不同的头（同一组内）中共享`KV`变换矩阵，减少重复计算，从而降低计算量和显存占用，尤其适合多头注意力的高效实现。
* ​**基于硬件特性的优化**​：例如FlashAttention，通过**分块计算**和**逐块保存中间结果**来减少全局内存的访问，充分利用GPU的内存层次结构，大幅提升计算速度和效率，尤其适用于长序列任务。

### FlashAttention

FlashAttention的核心是改进了自注意力机制的计算方式，特别是通过**分块计算**和**逐块保存中间结果**来减少了全局内存的访问量，，从而提升了计算速度。

### 为什么transformer的注意力分数要进行缩放

- 数值稳定性：在计算注意力分数时，Q和K的点积结果通常会有较大的数值范围，如果直接将这些较大的数值输入softmax函数，容易导致较小的梯度，进而影响训练的稳定性。通过缩放，可以控制softmax输入的范围，使得梯度在反向传播中更稳定。
- 方差归一化：假设Q和K矩阵的元素服从标准正态分布，它们的点积结果均值为0、方差为d_k。通过缩放系数1/√d_k，可以使方差归一到1，使得注意力分数的分布更稳定，有助于梯度传递，利于模型的训练效果。

### 词元切分方法：

Unigram词元分析、WordPiece， BPE

### 字节对编码

最开始的词表就是所有单个出现过的字符组成的

分割每个单词为字节序列，统计相邻字节对的出现频率，选择频率最高的字节对做为新的词元加入词表，并将全部单词中的该字节对合并为新的单一字节，尽量将词序列中的词切分成已知的
词元。




